# üöÄ Scraper PJE - Vers√£o Ultra Otimizada

## üìä Compara√ß√£o de Performance

### Vers√£o Original (`main_api.py`)
- ‚ùå Requisi√ß√µes sequenciais (uma por vez)
- ‚ùå Nova conex√£o TCP para cada requisi√ß√£o
- ‚ùå Log escrito no disco a cada requisi√ß√£o
- ‚ùå Delays fixos entre p√°ginas (2-3 segundos)
- ‚ùå Sem cache
- ‚è±Ô∏è Tempo estimado: **LENTO**

### Vers√£o Otimizada (`main_api_otimizado.py`)
- ‚úÖ Requisi√ß√µes paralelas (m√∫ltiplos tribunais + m√∫ltiplas p√°ginas)
- ‚úÖ Reuso de conex√µes HTTP com `requests.Session()`
- ‚úÖ Log em batch (escrito a cada 50 entradas)
- ‚úÖ Rate limiting inteligente (sem delays fixos desnecess√°rios)
- ‚úÖ Cache local (evita requisi√ß√µes repetidas)
- ‚è±Ô∏è Tempo estimado: **10x-50x MAIS R√ÅPIDO**

---

## ‚ö° Melhorias Implementadas

### 1. **requests.Session() - Reuso de Conex√µes**
```python
session = requests.Session()
session.headers.update(HEADERS)
```
- Mant√©m conex√µes HTTP keep-alive
- Elimina overhead de handshake TCP/TLS
- **Ganho: 30-50% mais r√°pido**

### 2. **ThreadPoolExecutor - Paralelismo em Dois N√≠veis**

#### N√≠vel 1: Paralelismo de Tribunais
```python
MAX_WORKERS_TRIBUNAIS = 5  # Processa 5 tribunais simultaneamente
```

#### N√≠vel 2: Paralelismo de P√°ginas
```python
MAX_WORKERS_PAGINAS = 10  # Busca 10 p√°ginas simultaneamente por tribunal
```

**Resultado:**
- Em vez de processar 1 p√°gina por vez, processa at√© 50 p√°ginas simultaneamente (5 tribunais √ó 10 p√°ginas)
- **Ganho: 10x-50x mais r√°pido dependendo da API**

### 3. **Log em Batch**
```python
LOG_BATCH_SIZE = 50  # Escreve a cada 50 logs
```
- Acumula logs em mem√≥ria
- Escreve no disco em lotes
- **Ganho: Elimina gargalo de I/O**

### 4. **Rate Limiting Inteligente (Token Bucket)**
```python
MAX_REQUESTS_PER_SECOND = 10
```
- Controla automaticamente a taxa de requisi√ß√µes
- Remove delays fixos desnecess√°rios
- Evita sobrecarga na API
- **Ganho: M√°xima velocidade sem erros 429**

### 5. **Sistema de Cache Local**
```python
CACHE_ENABLED = True
CACHE_DIR = "cache_api"
```
- Armazena respostas da API em JSON
- Evita requisi√ß√µes repetidas
- √ötil para testes e re-execu√ß√µes
- **Ganho: Requisi√ß√µes instant√¢neas para dados j√° baixados**

---

## üéØ Configura√ß√µes Recomendadas

### Para M√°xima Velocidade (Agressivo)
```python
MAX_WORKERS_TRIBUNAIS = 10
MAX_WORKERS_PAGINAS = 20
MAX_REQUESTS_PER_SECOND = 20
RATE_LIMIT_ENABLED = False  # ‚ö†Ô∏è Use com cuidado!
```

### Para Uso Seguro (Recomendado)
```python
MAX_WORKERS_TRIBUNAIS = 5
MAX_WORKERS_PAGINAS = 10
MAX_REQUESTS_PER_SECOND = 10
RATE_LIMIT_ENABLED = True
```

### Para Teste/Debug (Conservador)
```python
MAX_WORKERS_TRIBUNAIS = 2
MAX_WORKERS_PAGINAS = 5
MAX_REQUESTS_PER_SECOND = 5
RATE_LIMIT_ENABLED = True
LOG_ENABLED = True
```

---

## üìù Como Usar

### 1. Executar Vers√£o Otimizada
```bash
python main_api_otimizado.py
```

### 2. Ajustar Configura√ß√µes
Edite as vari√°veis no in√≠cio do arquivo:
```python
# Paralelismo
MAX_WORKERS_TRIBUNAIS = 5  # Quantos tribunais simultaneamente
MAX_WORKERS_PAGINAS = 10   # Quantas p√°ginas por tribunal

# Rate Limiting
MAX_REQUESTS_PER_SECOND = 10  # Requisi√ß√µes por segundo

# Cache
CACHE_ENABLED = True  # Ativa/desativa cache
```

### 3. Limpar Cache (se necess√°rio)
```bash
# Windows
rmdir /s cache_api

# Linux/Mac
rm -rf cache_api
```

---

## üìä Exemplo de Sa√≠da

```
üöÄ SCRAPER PJE - VERS√ÉO ULTRA OTIMIZADA
================================================================================

[‚öôÔ∏è] CONFIGURA√á√ïES:
    Per√≠odo: 2025-11-06 a 2025-11-10
    Tipo Comunica√ß√£o: Lista de distribui√ß√£o
    C√≥digo Classe: 12154

[‚ö°] OTIMIZA√á√ïES ATIVADAS:
    ‚úì requests.Session() - Reuso de conex√µes HTTP
    ‚úì ThreadPoolExecutor - 5 tribunais paralelos
    ‚úì Paralelismo de p√°ginas - 10 p√°ginas simult√¢neas
    ‚úì Rate Limiting - 10 req/s (ATIVADO)
    ‚úì Log em batch - 50 entradas (ATIVADO)
    ‚úì Cache local - ATIVADO

[üöÄ] Iniciando processamento paralelo de 1 tribunais...

================================================================================
üöÄ TRIBUNAL: TJSP - Tribunal de Justi√ßa de S√£o Paulo
================================================================================

  [üìä] Descobrindo total de p√°ginas...
  [‚ÑπÔ∏è] Total de itens: 10,000
  [‚ÑπÔ∏è] Total de p√°ginas: 100
  [‚ö°] Iniciando scraping paralelo com 10 workers...

  [‚ö°] Progresso: 100/100 p√°ginas (100.0%) | Filtrados: 1,234

================================================================================
[‚úÖ] TJSP CONCLU√çDO
================================================================================
  üìä ESTAT√çSTICAS:
      - P√°ginas processadas: 100
      - Itens totais: 10,000
      - Itens filtrados: 1,234
      - Taxa de filtro: 12.3%
      - Tempo total: 25.3s (0.4 min)  ‚ö° 50x mais r√°pido!
      - Velocidade: 395 itens/s
      - P√°ginas/s: 4.0
================================================================================
```

---

## ‚ö†Ô∏è Avisos Importantes

### Rate Limiting
- A API pode ter limites de taxa
- Se receber erros `429 Too Many Requests`:
  - Reduza `MAX_REQUESTS_PER_SECOND`
  - Reduza `MAX_WORKERS_PAGINAS`
  - Ative `RATE_LIMIT_ENABLED = True`

### Mem√≥ria
- Com muitos workers, o consumo de mem√≥ria aumenta
- Monitore o uso de RAM
- Reduza workers se necess√°rio

### Estabilidade
- Comece com configura√ß√µes conservadoras
- Aumente gradualmente os workers
- Monitore erros no log

---

## üîß Troubleshooting

### Erro: "Too Many Requests" (429)
```python
MAX_REQUESTS_PER_SECOND = 5  # Reduza
MAX_WORKERS_PAGINAS = 5      # Reduza
RATE_LIMIT_ENABLED = True    # Ative
```

### Erro: Timeout
```python
# Em fetch_page(), aumente:
response = session.get(url, timeout=60)  # Era 30
```

### Alto uso de mem√≥ria
```python
MAX_WORKERS_TRIBUNAIS = 2  # Reduza
MAX_WORKERS_PAGINAS = 5    # Reduza
```

### Cache ocupando muito espa√ßo
```bash
# Limpe o cache periodicamente
rm -rf cache_api
```

---

## üìà Benchmark Estimado

| Cen√°rio | Vers√£o Original | Vers√£o Otimizada | Ganho |
|---------|----------------|------------------|-------|
| 1 tribunal, 100 p√°ginas | ~200s | ~20s | **10x** |
| 5 tribunais, 100 p√°ginas cada | ~1000s | ~40s | **25x** |
| 10 tribunais, 500 p√°ginas cada | ~5000s | ~120s | **40x** |

*Tempos aproximados. Resultado real depende da lat√™ncia da API e configura√ß√µes.*

---

## üéì Pr√≥ximas Otimiza√ß√µes Poss√≠veis

### 1. Usar `asyncio` + `httpx`
- Ainda mais eficiente que threads
- Requer reescrita para async/await

### 2. Pool de Conex√µes Personalizado
```python
adapter = HTTPAdapter(
    pool_connections=100,
    pool_maxsize=100,
    max_retries=3
)
session.mount('https://', adapter)
```

### 3. Compress√£o de Dados
```python
HEADERS['Accept-Encoding'] = 'gzip, deflate'
```

### 4. Persist√™ncia de Session
- Salvar cookies entre execu√ß√µes
- Reduz ainda mais overhead

---

## üìû Suporte

Se encontrar problemas:
1. Verifique o arquivo `scraper_requests.log`
2. Ajuste as configura√ß√µes conforme troubleshooting
3. Teste com configura√ß√µes conservadoras primeiro

**Aproveite a velocidade! üöÄ**
