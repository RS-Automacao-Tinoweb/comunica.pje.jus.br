#!/usr/bin/env python3
"""
Scraper PJE - Vers√£o ULTRA OTIMIZADA
Performance m√°xima com paralelismo, cache e rate limiting inteligente!
"""

import json
import math
import requests
import time
import threading
import hashlib
from datetime import datetime
from urllib.parse import urlencode
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque

# Importar lista de tribunais
from tribunais import get_tribunais_por_tipo

# ===== CONFIGURA√á√ïES =====

# URL da API
API_BASE_URL = "https://comunicaapi.pje.jus.br/api/v1/comunicacao"

# Par√¢metros de busca
SEARCH_PARAMS = {
    "dataDisponibilizacaoInicio": "2025-11-06",
    "dataDisponibilizacaoFim": "2025-11-10"
}

# Filtros espec√≠ficos
FILTROS = {
    "tipoComunicacao": "Lista de distribui√ß√£o",
    "codigoClasse": "12154",
}

# Tipo de tribunais
TIPO_TRIBUNAL = "TODOS"
TRIBUNAIS_ESPECIFICOS = ["TJSP"]

# Pagina√ß√£o
ITEMS_POR_PAGINA = 100  # M√°ximo permitido pela API

# Diret√≥rios
OUTPUT_DIR = "resultados_api"
CACHE_DIR = "cache_api"
LOG_FILE = "scraper_requests.log"

# Headers para requisi√ß√£o
HEADERS = {
    "accept": "application/json, text/plain, */*",
    "sec-ch-ua": '"Chromium";v="142", "Google Chrome";v="142", "Not_A Brand";v="99"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# ===== CONFIGURA√á√ïES DE PERFORMANCE =====

# Paralelismo
MAX_WORKERS_TRIBUNAIS = 5  # Quantos tribunais processar simultaneamente
MAX_WORKERS_PAGINAS = 10   # Quantas p√°ginas buscar simultaneamente por tribunal

# Rate Limiting (requisi√ß√µes por segundo)
MAX_REQUESTS_PER_SECOND = 10  # Ajuste conforme necess√°rio
RATE_LIMIT_ENABLED = True

# Cache
CACHE_ENABLED = True  # Ativa cache para evitar requisi√ß√µes repetidas

# Log
LOG_BATCH_SIZE = 50  # Escreve logs a cada 50 entradas
LOG_ENABLED = True

# ===== SISTEMAS DE CONTROLE =====

# Session global para reuso de conex√µes HTTP
session = requests.Session()
session.headers.update(HEADERS)

# Buffer de logs (thread-safe)
log_buffer = deque()
log_lock = threading.Lock()

# Rate limiter simples (token bucket)
rate_limiter_lock = threading.Lock()
last_request_times = deque(maxlen=MAX_REQUESTS_PER_SECOND)


# ===== FUN√á√ïES AUXILIARES =====

def calcular_total_paginas(total_itens, itens_por_pagina):
    """Calcula total de p√°ginas baseado no count da API"""
    if itens_por_pagina <= 0:
        raise ValueError("itens_por_pagina deve ser maior que zero")
    if total_itens <= 0:
        return 0
    return math.ceil(total_itens / itens_por_pagina)


def gerar_cache_key(sigla_tribunal, pagina):
    """Gera chave √∫nica para cache baseada nos par√¢metros"""
    params_str = f"{sigla_tribunal}_{pagina}_{ITEMS_POR_PAGINA}_{SEARCH_PARAMS['dataDisponibilizacaoInicio']}_{SEARCH_PARAMS['dataDisponibilizacaoFim']}"
    return hashlib.md5(params_str.encode()).hexdigest()


def ler_cache(cache_key):
    """L√™ dados do cache se existir"""
    if not CACHE_ENABLED:
        return None
    
    cache_file = Path(CACHE_DIR) / f"{cache_key}.json"
    if cache_file.exists():
        try:
            with open(cache_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return None
    return None


def salvar_cache(cache_key, data):
    """Salva dados no cache"""
    if not CACHE_ENABLED:
        return
    
    Path(CACHE_DIR).mkdir(exist_ok=True)
    cache_file = Path(CACHE_DIR) / f"{cache_key}.json"
    try:
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
    except:
        pass


def rate_limit_wait():
    """Implementa rate limiting inteligente (token bucket)"""
    if not RATE_LIMIT_ENABLED:
        return
    
    with rate_limiter_lock:
        now = time.time()
        
        # Remove timestamps antigos (mais de 1 segundo)
        while last_request_times and now - last_request_times[0] > 1.0:
            last_request_times.popleft()
        
        # Se atingiu o limite, aguarda
        if len(last_request_times) >= MAX_REQUESTS_PER_SECOND:
            sleep_time = 1.0 - (now - last_request_times[0])
            if sleep_time > 0:
                time.sleep(sleep_time)
            last_request_times.popleft()
        
        last_request_times.append(time.time())


def log_request_batch(sigla_tribunal, pagina, url, params, response_data=None, error=None):
    """Adiciona log ao buffer (ser√° escrito em batch)"""
    if not LOG_ENABLED:
        return
    
    log_entry = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "tribunal": sigla_tribunal,
        "pagina": pagina,
        "url": url,
        "params": params,
        "status": "success" if not error else "error",
        "error": str(error) if error else None,
        "response_summary": {
            "total_disponivel": response_data.get("count") if response_data else None,
            "itens_retornados": len(response_data.get("items", [])) if response_data else 0
        } if response_data and not error else None
    }
    
    with log_lock:
        log_buffer.append(log_entry)
        
        # Flush se atingiu o tamanho do batch
        if len(log_buffer) >= LOG_BATCH_SIZE:
            flush_logs()


def flush_logs():
    """Escreve todos os logs pendentes no arquivo"""
    if not LOG_ENABLED:
        return
    
    with log_lock:
        if not log_buffer:
            return
        
        try:
            with open(LOG_FILE, "a", encoding="utf-8") as f:
                while log_buffer:
                    entry = log_buffer.popleft()
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"[!] Erro ao escrever logs: {e}")


def resolver_tribunais():
    """Resolve lista de tribunais a processar"""
    tribunais_disponiveis = get_tribunais_por_tipo(TIPO_TRIBUNAL)
    
    if not TRIBUNAIS_ESPECIFICOS:
        return tribunais_disponiveis
    
    siglas_normalizadas = [s.strip().upper() for s in TRIBUNAIS_ESPECIFICOS if s.strip()]
    
    if not siglas_normalizadas:
        return tribunais_disponiveis
    
    tribunais_map = {t["sigla"].upper(): t for t in tribunais_disponiveis}
    faltantes = [s for s in siglas_normalizadas if s not in tribunais_map]
    
    if faltantes:
        raise ValueError(
            f"Siglas inv√°lidas ({TIPO_TRIBUNAL}): {', '.join(sorted(faltantes))}"
        )
    
    return [tribunais_map[s] for s in siglas_normalizadas]


# ===== FUN√á√ïES DE SCRAPING =====

def fetch_page(sigla_tribunal, pagina=1):
    """
    Faz requisi√ß√£o para uma p√°gina espec√≠fica da API
    Usa Session para reuso de conex√µes e rate limiting inteligente
    """
    cache_key = gerar_cache_key(sigla_tribunal, pagina)
    
    # Tenta ler do cache primeiro
    cached_data = ler_cache(cache_key)
    if cached_data:
        return cached_data
    
    params = {
        "pagina": pagina,
        "itensPorPagina": ITEMS_POR_PAGINA,
        "siglaTribunal": sigla_tribunal,
        **SEARCH_PARAMS
    }
    
    url = f"{API_BASE_URL}?{urlencode(params)}"
    
    # Rate limiting
    rate_limit_wait()
    
    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # Salva no cache
        salvar_cache(cache_key, data)
        
        # Log
        log_request_batch(sigla_tribunal, pagina, url, params, response_data=data)
        
        return data
    
    except requests.exceptions.RequestException as e:
        log_request_batch(sigla_tribunal, pagina, url, params, error=str(e))
        return None


def filtrar_item(item):
    """Verifica se item atende aos filtros"""
    if FILTROS.get("tipoComunicacao"):
        if item.get("tipoComunicacao") != FILTROS["tipoComunicacao"]:
            return False
    
    if FILTROS.get("codigoClasse"):
        if str(item.get("codigoClasse")) != str(FILTROS["codigoClasse"]):
            return False
    
    return True


def extrair_dados_relevantes(item):
    """Extrai dados relevantes de um item"""
    return {
        "id": item.get("id"),
        "processo": item.get("numeroprocessocommascara"),
        "processo_sem_mascara": item.get("numero_processo"),
        "data_disponibilizacao": item.get("datadisponibilizacao"),
        "tribunal": item.get("siglaTribunal"),
        "tipo_comunicacao": item.get("tipoComunicacao"),
        "orgao": item.get("nomeOrgao"),
        "classe": item.get("nomeClasse"),
        "codigo_classe": item.get("codigoClasse"),
        "tipo_documento": item.get("tipoDocumento"),
        "meio": item.get("meiocompleto"),
        "link": item.get("link"),
        "hash": item.get("hash"),
        "texto": item.get("texto"),
        "partes": [
            {
                "nome": dest.get("nome"),
                "polo": dest.get("polo")
            }
            for dest in item.get("destinatarios", [])
        ],
        "advogados": [
            {
                "nome": adv.get("advogado", {}).get("nome"),
                "oab": adv.get("advogado", {}).get("numero_oab"),
                "uf": adv.get("advogado", {}).get("uf_oab")
            }
            for adv in item.get("destinatarioadvogados", [])
        ]
    }


def processar_pagina(sigla_tribunal, pagina):
    """Processa uma p√°gina individual (usado no paralelismo)"""
    data = fetch_page(sigla_tribunal, pagina)
    
    if not data or data.get("status") != "success":
        return []
    
    items = data.get("items", [])
    resultados = []
    
    for item in items:
        if filtrar_item(item):
            dados = extrair_dados_relevantes(item)
            resultados.append(dados)
    
    return resultados


def scrape_tribunal_api_paralelo(tribunal):
    """
    Vers√£o OTIMIZADA com paralelismo de p√°ginas
    Busca m√∫ltiplas p√°ginas simultaneamente
    """
    sigla = tribunal["sigla"]
    nome = tribunal["nome"]
    
    print(f"\n{'='*80}")
    print(f"üöÄ TRIBUNAL: {sigla} - {nome}")
    print(f"{'='*80}\n")
    
    tempo_inicio = time.time()
    
    # Primeira requisi√ß√£o para descobrir total de p√°ginas
    print(f"  [üìä] Descobrindo total de p√°ginas...")
    data_primeira = fetch_page(sigla, 1)
    
    if not data_primeira or data_primeira.get("status") != "success":
        print(f"  [!] Erro ao buscar primeira p√°gina")
        return []
    
    count_total = data_primeira.get("count", 0)
    total_paginas = calcular_total_paginas(count_total, ITEMS_POR_PAGINA)
    
    print(f"  [‚ÑπÔ∏è] Total de itens: {count_total:,}")
    print(f"  [‚ÑπÔ∏è] Total de p√°ginas: {total_paginas:,}")
    print(f"  [‚ö°] Iniciando scraping paralelo com {MAX_WORKERS_PAGINAS} workers...\n")
    
    if total_paginas == 0:
        return []
    
    # Processa primeira p√°gina
    all_results = processar_pagina(sigla, 1)
    
    # Processa p√°ginas restantes em paralelo
    if total_paginas > 1:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS_PAGINAS) as executor:
            # Submete todas as p√°ginas
            futures = {
                executor.submit(processar_pagina, sigla, pag): pag 
                for pag in range(2, total_paginas + 1)
            }
            
            # Coleta resultados conforme completam
            paginas_processadas = 1
            for future in as_completed(futures):
                pagina_num = futures[future]
                try:
                    resultados = future.result()
                    all_results.extend(resultados)
                    paginas_processadas += 1
                    
                    # Progress
                    progresso = (paginas_processadas / total_paginas) * 100
                    print(f"  [‚ö°] Progresso: {paginas_processadas}/{total_paginas} p√°ginas ({progresso:.1f}%) | Filtrados: {len(all_results):,}", end="\r")
                
                except Exception as e:
                    print(f"\n  [!] Erro na p√°gina {pagina_num}: {e}")
    
    tempo_total = time.time() - tempo_inicio
    
    print(f"\n\n{'='*80}")
    print(f"[‚úÖ] {sigla} CONCLU√çDO")
    print(f"{'='*80}")
    print(f"  üìä ESTAT√çSTICAS:")
    print(f"      - P√°ginas processadas: {total_paginas:,}")
    print(f"      - Itens totais: {count_total:,}")
    print(f"      - Itens filtrados: {len(all_results):,}")
    print(f"      - Taxa de filtro: {(len(all_results)/count_total*100 if count_total > 0 else 0):.1f}%")
    print(f"      - Tempo total: {tempo_total:.1f}s ({tempo_total/60:.1f} min)")
    print(f"      - Velocidade: {count_total/tempo_total:.0f} itens/s")
    print(f"      - P√°ginas/s: {total_paginas/tempo_total:.1f}")
    print(f"{'='*80}\n")
    
    return all_results


def processar_tribunal(tribunal):
    """Wrapper para processar tribunal (usado no paralelismo de tribunais)"""
    try:
        sigla = tribunal["sigla"]
        resultados = scrape_tribunal_api_paralelo(tribunal)
        return sigla, resultados, tribunal["nome"]
    except Exception as e:
        print(f"[!] Erro ao processar {tribunal['sigla']}: {e}")
        return tribunal["sigla"], [], tribunal["nome"]


def main():
    """
    Main OTIMIZADO com paralelismo de tribunais e p√°ginas
    """
    print("="*80)
    print("üöÄ SCRAPER PJE - VERS√ÉO ULTRA OTIMIZADA")
    print("="*80)
    print()
    
    # Limpa log anterior
    if Path(LOG_FILE).exists():
        Path(LOG_FILE).unlink()
    
    # Mostra configura√ß√µes
    print("[‚öôÔ∏è] CONFIGURA√á√ïES:")
    print(f"    Per√≠odo: {SEARCH_PARAMS['dataDisponibilizacaoInicio']} a {SEARCH_PARAMS['dataDisponibilizacaoFim']}")
    print(f"    Tipo Comunica√ß√£o: {FILTROS.get('tipoComunicacao', 'TODOS')}")
    print(f"    C√≥digo Classe: {FILTROS.get('codigoClasse', 'TODOS')}")
    print(f"    Tipo Tribunal: {TIPO_TRIBUNAL}")
    print(f"    Tribunais espec√≠ficos: {TRIBUNAIS_ESPECIFICOS or 'Todos'}")
    print()
    
    print("[‚ö°] OTIMIZA√á√ïES ATIVADAS:")
    print(f"    ‚úì requests.Session() - Reuso de conex√µes HTTP")
    print(f"    ‚úì ThreadPoolExecutor - {MAX_WORKERS_TRIBUNAIS} tribunais paralelos")
    print(f"    ‚úì Paralelismo de p√°ginas - {MAX_WORKERS_PAGINAS} p√°ginas simult√¢neas")
    print(f"    ‚úì Rate Limiting - {MAX_REQUESTS_PER_SECOND} req/s {'(ATIVADO)' if RATE_LIMIT_ENABLED else '(DESATIVADO)'}")
    print(f"    ‚úì Log em batch - {LOG_BATCH_SIZE} entradas {'(ATIVADO)' if LOG_ENABLED else '(DESATIVADO)'}")
    print(f"    ‚úì Cache local - {'ATIVADO' if CACHE_ENABLED else 'DESATIVADO'}")
    print()
    
    # Cria diret√≥rios
    Path(OUTPUT_DIR).mkdir(exist_ok=True)
    if CACHE_ENABLED:
        Path(CACHE_DIR).mkdir(exist_ok=True)
    
    # Obt√©m tribunais
    tribunais = resolver_tribunais()
    print(f"[üìã] Tribunais a processar: {len(tribunais)}")
    print()
    
    tempo_inicio_total = time.time()
    resultados_consolidados = {}
    total_geral = 0
    
    # Processa tribunais em paralelo
    print(f"[üöÄ] Iniciando processamento paralelo de {len(tribunais)} tribunais...\n")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_TRIBUNAIS) as executor:
        futures = {executor.submit(processar_tribunal, t): t for t in tribunais}
        
        for future in as_completed(futures):
            tribunal = futures[future]
            try:
                sigla, resultados, nome = future.result()
                
                if resultados:
                    # Salva resultados individuais
                    output_file = Path(OUTPUT_DIR) / f"{sigla}.json"
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(resultados, f, ensure_ascii=False, indent=2)
                    
                    resultados_consolidados[sigla] = {
                        "tribunal": nome,
                        "total_registros": len(resultados),
                        "registros": resultados
                    }
                    total_geral += len(resultados)
                    print(f"[üíæ] {sigla}: {len(resultados):,} registros salvos")
                else:
                    print(f"[!] {sigla}: Nenhum resultado")
            
            except Exception as e:
                print(f"[!] Erro ao processar {tribunal['sigla']}: {e}")
    
    # Flush logs pendentes
    flush_logs()
    
    tempo_total_execucao = time.time() - tempo_inicio_total
    
    # Resumo final
    print("\n" + "="*80)
    print("üìä RESUMO FINAL")
    print("="*80)
    print(f"Total de tribunais processados: {len(resultados_consolidados)}")
    print(f"Total geral de registros: {total_geral:,}")
    print(f"Tempo total de execu√ß√£o: {tempo_total_execucao:.1f}s ({tempo_total_execucao/60:.1f} min)")
    print(f"Velocidade m√©dia: {total_geral/tempo_total_execucao:.0f} registros/s")
    print()
    
    # Detalhes por tribunal
    for sigla, dados in resultados_consolidados.items():
        print(f"  - {sigla}: {dados['total_registros']:,} registros")
    
    # Salva consolidado
    consolidado_file = Path(OUTPUT_DIR) / "consolidado.json"
    with open(consolidado_file, "w", encoding="utf-8") as f:
        json.dump(resultados_consolidados, f, ensure_ascii=False, indent=2)
    print(f"\n[üíæ] Consolidado salvo: {consolidado_file}")
    
    # Salva resumo
    resumo = {
        "data_execucao": datetime.now().isoformat(),
        "parametros_busca": SEARCH_PARAMS,
        "filtros": FILTROS,
        "tipo_tribunal": TIPO_TRIBUNAL,
        "total_tribunais": len(resultados_consolidados),
        "total_registros": total_geral,
        "tempo_execucao_segundos": tempo_total_execucao,
        "velocidade_registros_por_segundo": total_geral / tempo_total_execucao if tempo_total_execucao > 0 else 0,
        "otimizacoes": {
            "session_reuso": True,
            "paralelismo_tribunais": MAX_WORKERS_TRIBUNAIS,
            "paralelismo_paginas": MAX_WORKERS_PAGINAS,
            "rate_limiting": RATE_LIMIT_ENABLED,
            "cache": CACHE_ENABLED,
            "log_batch": LOG_ENABLED
        },
        "tribunais": {
            sigla: {
                "nome": dados["tribunal"],
                "total": dados["total_registros"]
            }
            for sigla, dados in resultados_consolidados.items()
        }
    }
    
    resumo_file = Path(OUTPUT_DIR) / "resumo.json"
    with open(resumo_file, "w", encoding="utf-8") as f:
        json.dump(resumo, f, ensure_ascii=False, indent=2)
    print(f"[üíæ] Resumo salvo: {resumo_file}")
    
    print("\n" + "="*80)
    print("‚úÖ CONCLU√çDO COM SUCESSO!")
    print("="*80)


if __name__ == "__main__":
    main()
